{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML, Image\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "import os\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db9d0ef867f42f187905ffcb97f3614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b9c4bca1314b75974e7fb8dab5717d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36af9ccdeb3b4c45bb10d668678616ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9624842af1343bf8f7c50d70eac6d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32ac5f7d12947628646bd7aea3aa6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/627M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the layers of TFRobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_logits(logits, indices_to_filter=None, fill_with=-np.inf): # different behavior for [-1] on gpu and cpu TODO add ability to prefer certain logits\n",
    "    if indices_to_filter is None:\n",
    "        return logits\n",
    "    else:\n",
    "        indices_to_filter = tf.expand_dims(indices_to_filter, axis=1)\n",
    "        filters = tf.fill([len(indices_to_filter)], fill_with)\n",
    "        filtered_logits = tf.tensor_scatter_nd_update(logits, indices_to_filter, filters)    \n",
    "        return filtered_logits\n",
    "    \n",
    "\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "def sample_from_logits(logits, num_samples=1):\n",
    "    sample = tf.random.categorical([logits], num_samples, dtype=tf.int32)\n",
    "    sample = tf.reshape(sample, shape=(tf.shape(sample)[1],))\n",
    "    return sample\n",
    "\n",
    "def temperature(logits, temp):\n",
    "    return logits / temp\n",
    "\n",
    "def top_k_filter(logits, top_k):\n",
    "    indices_to_filter = tf.math.top_k(-logits, k=len(logits)-top_k).indices\n",
    "    print(indices_to_filter)\n",
    "    return filter_logits(logits, indices_to_filter=indices_to_filter)\n",
    "\n",
    "def top_p_filter(logits, top_p):\n",
    "    logits = tf.squeeze(logits)\n",
    "    indices_sorted = tf.argsort(logits, direction=\"DESCENDING\")\n",
    "    logits_sorted = tf.gather(logits, indices_sorted)\n",
    "    probs_sorted = tf.nn.softmax(logits_sorted, axis=-1)\n",
    "    cutoff_index = tf.argmax(tf.cumsum(probs_sorted)>top_p)\n",
    "    indices_to_filter = indices_sorted[cutoff_index+1:]\n",
    "    return filter_logits(logits, indices_to_filter=indices_to_filter)\n",
    "\n",
    "def filter_masked_logits(logits, top_k=0, top_p=1.0, temp=1.0, custom=None):\n",
    "    logits = filter_logits(logits, indices_to_filter=custom)\n",
    "    if temp!=1.0:\n",
    "        logits = temperature(logits, temp=temp)\n",
    "    if top_k>0:\n",
    "        logits = top_k_filter(logits, top_k=top_k)\n",
    "    if top_p<1.0:\n",
    "        logits = top_p_filter(logits, top_p=top_p)\n",
    "    return logits\n",
    "\n",
    "def tokenize_sentence(sentence, tokenizer):\n",
    "        tokenized_tensor = tokenizer(sentence, return_tensors='tf')['input_ids']\n",
    "        tokens = tf.squeeze(tokenized_tensor)\n",
    "        return tokens\n",
    "\n",
    "def frozen_indices(sentence, tokenizer, separator=\"&\"):\n",
    "    indices = [(np.array(match.span()) - i) for i,match in enumerate(re.finditer(separator, sentence))]\n",
    "    indices = [np.concatenate(thing)[[0,-1]] for thing in list(pairwise(indices))]\n",
    "    indices = [(thing - np.array((0,1))) for thing in indices]\n",
    "    mapping = tokenizer(sentence.replace(\"&\", \"\"), return_offsets_mapping=True, return_attention_mask=False, return_token_type_ids=False)['offset_mapping'][1:-1]\n",
    "    tokenized = tokenizer.tokenize(sentence.replace(\"&\", \"\"))\n",
    "    r = len(tokenized)+1\n",
    "    mapping = dict(zip(mapping, list(range(1,r))))\n",
    "    all_ix = {}\n",
    "    for span in indices:\n",
    "        a,b = span\n",
    "        try:\n",
    "            i,j = {k:v for k,v in mapping.items() if k[0]==a or k[1]==b}.values()\n",
    "            ix_span = list(range(i, j+1))\n",
    "        except:\n",
    "            ix_span = {k:v for k,v in mapping.items() if k[0]==a or k[1]==b}.values()\n",
    "        all_ix.update({i:tokenized[i-1] for i in ix_span})\n",
    "    return set(all_ix.keys()), sentence.replace(\"&\", \"\")\n",
    "    \n",
    "def energy_norm(tokens, tokenizer, model):\n",
    "    ix = list(range(1,len(tokens)-1))\n",
    "    mask_token = tf.constant([tokenizer.mask_token_id])\n",
    "    log_probs = tf.constant([0.])\n",
    "    for i in ix:\n",
    "        indices = tf.constant([[i]])\n",
    "        updates = mask_token\n",
    "        masked = tf.tensor_scatter_nd_update(tokens, indices, updates)   \n",
    "        masked_token_logits = tf.squeeze(model(tf.expand_dims(masked, axis=0))[0])[i]       \n",
    "        log_prob = -tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tokens[i], logits=masked_token_logits)\n",
    "        log_prob = tf.expand_dims(log_prob, axis=0)\n",
    "        log_probs = tf.concat([log_probs, log_prob], axis=0)\n",
    "    return tf.reduce_sum(log_probs) / len(log_probs)\n",
    "\n",
    "def energy_raw(tokens, tokenizer, model):\n",
    "    ix = list(range(1,len(tokens)-1))\n",
    "    mask_token = tf.constant([tokenizer.mask_token_id])\n",
    "    logit_values = tf.constant([0.])\n",
    "    for i in ix:\n",
    "        indices = tf.constant([[i]])\n",
    "        updates = mask_token\n",
    "        masked = tf.tensor_scatter_nd_update(tokens, indices, updates)\n",
    "        masked_token_logits = tf.squeeze(model(tf.expand_dims(masked, axis=0))[0])[i]       \n",
    "        logit_value = masked_token_logits[tokens[i]]\n",
    "        logit_value = tf.expand_dims(logit_value, axis=0)\n",
    "        logit_values = tf.concat([logit_values, logit_value], axis=0)\n",
    "    return tf.reduce_sum(logit_values) / len(logit_values)\n",
    "\n",
    "def metropolis_hastings_one_iteration(tokens, mask_token, tokenizer, model, frozen=set(), \n",
    "                                      randomized=True, mode='norm', top_k=0, top_p=1.0, temp=1.0, custom=None):\n",
    "    ix = list(range(1,len(tokens)-1))\n",
    "    ix = list(set(ix)-frozen)\n",
    "#     print(dict(zip(ix, tokens.numpy())))\n",
    "    if randomized:\n",
    "        np.random.shuffle(ix)\n",
    "    for i in ix:\n",
    "        masked = tf.tensor_scatter_nd_update(tokens, tf.constant([[i]]), mask_token)\n",
    "        masked_token_logits = tf.squeeze(model(tf.expand_dims(masked, axis=0))[0])[i]\n",
    "        masked_token_logits = filter_masked_logits(masked_token_logits, top_k=top_k, top_p=top_p, temp=temp, custom=custom) # custom = custom[i]??\n",
    "        candidate_token = sample_from_logits(masked_token_logits, num_samples=1)\n",
    "        candidate_tokens = tf.tensor_scatter_nd_update(tokens, tf.constant([[i]]), candidate_token)\n",
    "        if mode=='norm':\n",
    "            energy = energy_norm\n",
    "        elif mode=='raw':\n",
    "            energy = energy_raw\n",
    "        E_old = energy(tokens, tokenizer, model)\n",
    "        E_new = energy(candidate_tokens, tokenizer, model)\n",
    "        q_old = tf.nn.softmax(masked_token_logits)[tokens[i]]\n",
    "        q_new = tf.nn.softmax(masked_token_logits)[tf.squeeze(candidate_token)]\n",
    "        accept = tf.minimum(1., (E_new/E_old)*(q_old/q_new))\n",
    "        u = tf.random.uniform(shape=())\n",
    "        if tokens[i] == mask_token:\n",
    "            accept=1.\n",
    "        tokens =  candidate_tokens if u <= accept else tokens\n",
    "    return tokens\n",
    "\n",
    "def generate(sentence, tokenizer, model, num_iterations=10, randomized=True, mode='norm', \n",
    "             top_k=0, top_p=1.0, temp=1.0, custom=None, print_generated=True):\n",
    "    frozen, sentence = frozen_indices(sentence, tokenizer)\n",
    "    new_tokens = tokenize_sentence(sentence, tokenizer)\n",
    "    #CREATE FILTERING DICT HERE\n",
    "    token_lists = [new_tokens]\n",
    "    mask_token = tf.constant([tokenizer.mask_token_id])\n",
    "    for i in range(num_iterations):\n",
    "        new_tokens = metropolis_hastings_one_iteration(new_tokens, mask_token, tokenizer, model, frozen=frozen, randomized=randomized, mode=mode, top_k=top_k, top_p=top_p, temp=temp, custom=custom)\n",
    "        token_lists.append(new_tokens)\n",
    "        if print_generated:\n",
    "            print(tokenizer.decode(new_tokens))\n",
    "    return token_lists\n",
    "\n",
    "def lipogram_filter(tokenizer, letter=\"e\"):\n",
    "    vocabulary = {tokenizer.convert_tokens_to_string(k):v for k,v in tokenizer.vocab.items()}\n",
    "    return [vocabulary[subword] for subword in vocabulary if (letter in subword or letter.capitalize() in subword)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {tokenizer.convert_tokens_to_string(k):v for k,v in tokenizer.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[subword for subword in vocabulary if (\"sex\" in subword or \"Sex\" in subword)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0, 2: 50264, 3: 50264, 4: 50264}\n",
      "<s>7.1.</s>\n",
      "{1: 0, 2: 406, 3: 4, 4: 134}\n",
      "<s>7. 8.</s>\n",
      "{1: 0, 2: 406, 3: 4, 4: 290}\n",
      "<s>7. Introduction.</s>\n",
      "{1: 0, 2: 406, 3: 4, 4: 24474}\n",
      "<s>7. California legislature</s>\n",
      "{1: 0, 2: 406, 3: 4, 4: 886}\n",
      "<s>C. Iowa legislature</s>\n",
      "CPU times: user 20.7 s, sys: 48.7 ms, total: 20.8 s\n",
      "Wall time: 20.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s><mask><mask><mask>.</s>',\n",
       " '<s>7.1.</s>',\n",
       " '<s>7. 8.</s>',\n",
       " '<s>7. Introduction.</s>',\n",
       " '<s>7. California legislature</s>',\n",
       " '<s>C. Iowa legislature</s>']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sentence=\"<mask><mask><mask>.\"\n",
    "tokens = generate(sentence, tokenizer, model, randomized=False, mode='norm', num_iterations=5, top_p=0.8, temp=1.0, print_generated=True)\n",
    "tokenizer.batch_decode(tf.stack(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>In this prison of flesh and blood, they will be liberated.</s>\n",
      "<s>In this prison of flesh and blood, they must be alive.</s>\n",
      "<s>In this prison of flesh and blood, they might stay alive.</s>\n",
      "<s>In this prison of flesh and blood, they might stay alive.</s>\n",
      "<s>In this prison of flesh and blood, they might remain apart.</s>\n",
      "CPU times: user 2min 16s, sys: 404 ms, total: 2min 16s\n",
      "Wall time: 2min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>In this prison of flesh<mask><mask><mask><mask><mask><mask><mask><mask></s>',\n",
       " '<s>In this prison of flesh and blood, they will be liberated.</s>',\n",
       " '<s>In this prison of flesh and blood, they must be alive.</s>',\n",
       " '<s>In this prison of flesh and blood, they might stay alive.</s>',\n",
       " '<s>In this prison of flesh and blood, they might stay alive.</s>',\n",
       " '<s>In this prison of flesh and blood, they might remain apart.</s>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sentence=\"&In this prison of flesh& <mask><mask><mask><mask><mask><mask><mask><mask>\"\n",
    "tokens = generate(sentence, tokenizer, model, randomized=False, mode='raw', num_iterations=5, top_p=0.8, temp=1.05, print_generated=True)\n",
    "tokenizer.batch_decode(tf.stack(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentence=\"&In this prison of flesh& <mask><mask><mask><mask><mask><mask><mask><mask>\"\n",
    "tokens = generate(sentence, tokenizer, model, num_iterations=20, top_p=0.8, temp=1.05, print_generated=True)\n",
    "tokenizer.batch_decode(tf.stack(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>I traveled far and wide while I was learning to fly.</s>\n",
      "<s>I traveled far and wide while I was learning to read.</s>\n",
      "<s>I traveled far and wide while I was able to spare.</s>\n",
      "<s>I traveled far and wide while I was able to rest.</s>\n",
      "<s>I traveled far and wide while I was able to survive.</s>\n",
      "<s>I traveled far and wide while I was able to drive.</s>\n",
      "<s>I traveled far and wide while I was able to teach.</s>\n",
      "<s>I traveled far and wide while I was teaching to teach.</s>\n",
      "<s>I traveled far and wide while I was able to teach.</s>\n",
      "<s>I traveled far and wide while I was preparing to teach.</s>\n",
      "<s>I traveled far and wide as I was preparing to teach.</s>\n",
      "<s>I traveled far and wide as I was preparing to teach.</s>\n",
      "<s>I traveled far and wide as I was preparing to teach.</s>\n",
      "<s>I traveled far and wide as I was preparing to teach.</s>\n",
      "<s>I traveled far and wide while I was preparing to teach.</s>\n",
      "<s>I traveled far and wide when I was preparing to travel.</s>\n",
      "<s>I traveled far and fast while I was preparing to travel.</s>\n",
      "<s>I traveled far and fast when I was preparing to ship.</s>\n",
      "<s>I traveled far and fast when I was preparing to ship.</s>\n",
      "<s>I traveled far and fast. I was determined to ship.</s>\n",
      "CPU times: user 7min 58s, sys: 2.35 s, total: 8min\n",
      "Wall time: 8min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>I traveled far and<mask><mask><mask><mask><mask><mask><mask><mask></s>',\n",
       " '<s>I traveled far and wide while I was learning to fly.</s>',\n",
       " '<s>I traveled far and wide while I was learning to read.</s>',\n",
       " '<s>I traveled far and wide while I was able to spare.</s>',\n",
       " '<s>I traveled far and wide while I was able to rest.</s>',\n",
       " '<s>I traveled far and wide while I was able to survive.</s>',\n",
       " '<s>I traveled far and wide while I was able to drive.</s>',\n",
       " '<s>I traveled far and wide while I was able to teach.</s>',\n",
       " '<s>I traveled far and wide while I was teaching to teach.</s>',\n",
       " '<s>I traveled far and wide while I was able to teach.</s>',\n",
       " '<s>I traveled far and wide while I was preparing to teach.</s>',\n",
       " '<s>I traveled far and wide as I was preparing to teach.</s>',\n",
       " '<s>I traveled far and wide as I was preparing to teach.</s>',\n",
       " '<s>I traveled far and wide as I was preparing to teach.</s>',\n",
       " '<s>I traveled far and wide as I was preparing to teach.</s>',\n",
       " '<s>I traveled far and wide while I was preparing to teach.</s>',\n",
       " '<s>I traveled far and wide when I was preparing to travel.</s>',\n",
       " '<s>I traveled far and fast while I was preparing to travel.</s>',\n",
       " '<s>I traveled far and fast when I was preparing to ship.</s>',\n",
       " '<s>I traveled far and fast when I was preparing to ship.</s>',\n",
       " '<s>I traveled far and fast. I was determined to ship.</s>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sentence=\"&I traveled far and& <mask><mask><mask><mask><mask><mask><mask><mask>\"\n",
    "tokens = generate(sentence, tokenizer, model, num_iterations=20, top_p=0.8, temp=1.05, print_generated=True)\n",
    "tokenizer.batch_decode(tf.stack(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for some reason, they decided to let me die.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for some reason my destiny refused to let me die.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for some reason my destiny decided to let me die.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for some reason, God decided to let me die.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for some reason, they decided to let me die.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for some reason, fate decided to let me die.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for some reason, fate chose to let me die.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for some reason, he chose to let me die.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for whatever reason, people chose to let me fly.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for whatever reason, people chose to let me live.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for some reason, people chose to let me live.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for whatever reason, Mom chose to let me stay.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for whatever reason, Mom chose to let me stay.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for some reason, Mom chose to let me stay.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for some reason, it chose to let me stay.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for some reason, it chose to let me stay.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for whatever reason, it chose to let me stay.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for whatever reason, it chose to let me stay.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for whatever reason, it chose to let me stay.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But for whatever reason, it chose to let me stay.</s>\n",
      "CPU times: user 21min 9s, sys: 3.73 s, total: 21min 13s\n",
      "Wall time: 21min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>I traveled far and wide while I was learning to fly,\\nBut<mask><mask><mask><mask><mask><mask><mask><mask><mask> die.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for some reason, they decided to let me die.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for some reason my destiny refused to let me die.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for some reason my destiny decided to let me die.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for some reason, God decided to let me die.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for some reason, they decided to let me die.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for some reason, fate decided to let me die.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for some reason, fate chose to let me die.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for some reason, he chose to let me die.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for whatever reason, people chose to let me fly.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for whatever reason, people chose to let me live.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for some reason, people chose to let me live.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for whatever reason, Mom chose to let me stay.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for whatever reason, Mom chose to let me stay.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for some reason, Mom chose to let me stay.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for some reason, it chose to let me stay.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for some reason, it chose to let me stay.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for whatever reason, it chose to let me stay.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for whatever reason, it chose to let me stay.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for whatever reason, it chose to let me stay.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut for whatever reason, it chose to let me stay.</s>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sentence=\"&I traveled far and wide while I was learning to fly,\\nBut& <mask><mask><mask><mask><mask><mask><mask><mask><mask>& die.&\"\n",
    "tokens = generate(sentence, tokenizer, model, num_iterations=20, top_p=0.8, temp=1.05, print_generated=True)\n",
    "tokenizer.batch_decode(tf.stack(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add filter for question words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how can be, and I said I flew.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how could blame you and I said I flew.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how to blame you and I said I flew.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how to blame yourself and have said you flew.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how to blame others and have said you flew.</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how to blame others and have said you flew.\"</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how to blame others and have said you flew?\"</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how to blame others and they said you flew?</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how to blame others and they said you flew?\"</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how you blame yourself and they said you flew?</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how many blame airplanes and countries said you sailed?</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how many blame airplanes and countries said you sailed?\"</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how many blame airplanes and countries said you sailed?\"</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how could blame Europe and countries said you wrong?\"</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how could blame Europe and countries said you wrong?</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how dare blame Europe and countries said you wrong?\"</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how dare blame Europe and countries said you wrong?\"</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how dare blame Europe and countries said you wrong?</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how dare blame Europe and countries said you wrong?</s>\n",
      "<s>I traveled far and wide while I was learning to fly,\n",
      "But how to blame Europe and countries said you wrong?</s>\n",
      "CPU times: user 17min 10s, sys: 2.65 s, total: 17min 13s\n",
      "Wall time: 17min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>I traveled far and wide while I was learning to fly,\\nBut how<mask><mask><mask><mask><mask><mask><mask><mask><mask></s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how can be, and I said I flew.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how could blame you and I said I flew.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how to blame you and I said I flew.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how to blame yourself and have said you flew.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how to blame others and have said you flew.</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how to blame others and have said you flew.\"</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how to blame others and have said you flew?\"</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how to blame others and they said you flew?</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how to blame others and they said you flew?\"</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how you blame yourself and they said you flew?</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how many blame airplanes and countries said you sailed?</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how many blame airplanes and countries said you sailed?\"</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how many blame airplanes and countries said you sailed?\"</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how could blame Europe and countries said you wrong?\"</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how could blame Europe and countries said you wrong?</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how dare blame Europe and countries said you wrong?\"</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how dare blame Europe and countries said you wrong?\"</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how dare blame Europe and countries said you wrong?</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how dare blame Europe and countries said you wrong?</s>',\n",
       " '<s>I traveled far and wide while I was learning to fly,\\nBut how to blame Europe and countries said you wrong?</s>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sentence=\"&I traveled far and wide while I was learning to fly,\\nBut how& <mask><mask><mask><mask><mask><mask><mask><mask><mask>\"\n",
    "tokens = generate(sentence, tokenizer, model, num_iterations=20, top_p=0.85, temp=0.95, print_generated=True)\n",
    "tokenizer.batch_decode(tf.stack(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>This pain is excruciating.</s>\n",
      "<s>This pain is excruciating.</s>\n",
      "<s>This pain is excruciating.</s>\n",
      "<s>This pain is unbearable.</s>\n",
      "<s>This pain is excruciating.</s>\n",
      "<s>This pain is unbearable.</s>\n",
      "<s>This pain is unbearable.</s>\n",
      "<s>This pain is unbearable.</s>\n",
      "<s>This pain is unbearable.</s>\n",
      "<s>This pain is excruciating.</s>\n",
      "<s>This pain is unbearable.</s>\n",
      "<s>This pain is excruciating.</s>\n",
      "<s>This pain is excruciating.</s>\n",
      "<s>This pain is excruciating.</s>\n",
      "<s>This pain is excruciating.</s>\n",
      "<s>This pain is excruciating.</s>\n",
      "<s>This pain is excruciating.</s>\n",
      "<s>This pain is excruciating.</s>\n",
      "<s>This pain is excruciating.</s>\n",
      "<s>This pain is unbearable.</s>\n",
      "CPU times: user 37.9 s, sys: 98.3 ms, total: 38 s\n",
      "Wall time: 37.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>This pain is<mask>.</s>',\n",
       " '<s>This pain is excruciating.</s>',\n",
       " '<s>This pain is excruciating.</s>',\n",
       " '<s>This pain is excruciating.</s>',\n",
       " '<s>This pain is unbearable.</s>',\n",
       " '<s>This pain is excruciating.</s>',\n",
       " '<s>This pain is unbearable.</s>',\n",
       " '<s>This pain is unbearable.</s>',\n",
       " '<s>This pain is unbearable.</s>',\n",
       " '<s>This pain is unbearable.</s>',\n",
       " '<s>This pain is excruciating.</s>',\n",
       " '<s>This pain is unbearable.</s>',\n",
       " '<s>This pain is excruciating.</s>',\n",
       " '<s>This pain is excruciating.</s>',\n",
       " '<s>This pain is excruciating.</s>',\n",
       " '<s>This pain is excruciating.</s>',\n",
       " '<s>This pain is excruciating.</s>',\n",
       " '<s>This pain is excruciating.</s>',\n",
       " '<s>This pain is excruciating.</s>',\n",
       " '<s>This pain is excruciating.</s>',\n",
       " '<s>This pain is unbearable.</s>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sentence=\"&This pain& is <mask>&.&\"\n",
    "tokens = generate(sentence, tokenizer, model, num_iterations=20, top_p=0.1, temp=1.05, print_generated=True)\n",
    "tokenizer.batch_decode(tf.stack(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>This pain is normal.</s>\n",
      "<s>This pain is normal.</s>\n",
      "<s>This pain is normal.</s>\n",
      "<s>This pain is normal.</s>\n",
      "<s>This pain is normal.</s>\n",
      "<s>This pain is familiar.</s>\n",
      "<s>This pain is familiar.</s>\n",
      "<s>This pain is familiar.</s>\n",
      "<s>This pain is normal.</s>\n",
      "<s>This pain is normal.</s>\n",
      "<s>This pain is familiar.</s>\n",
      "<s>This pain is familiar.</s>\n",
      "<s>This pain is normal.</s>\n",
      "<s>This pain is normal.</s>\n",
      "<s>This pain is familiar.</s>\n",
      "<s>This pain is familiar.</s>\n",
      "<s>This pain is normal.</s>\n",
      "<s>This pain is normal.</s>\n",
      "<s>This pain is familiar.</s>\n",
      "<s>This pain is familiar.</s>\n",
      "CPU times: user 39.5 s, sys: 177 ms, total: 39.7 s\n",
      "Wall time: 39.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>This pain is<mask>.</s>',\n",
       " '<s>This pain is normal.</s>',\n",
       " '<s>This pain is normal.</s>',\n",
       " '<s>This pain is normal.</s>',\n",
       " '<s>This pain is normal.</s>',\n",
       " '<s>This pain is normal.</s>',\n",
       " '<s>This pain is familiar.</s>',\n",
       " '<s>This pain is familiar.</s>',\n",
       " '<s>This pain is familiar.</s>',\n",
       " '<s>This pain is normal.</s>',\n",
       " '<s>This pain is normal.</s>',\n",
       " '<s>This pain is familiar.</s>',\n",
       " '<s>This pain is familiar.</s>',\n",
       " '<s>This pain is normal.</s>',\n",
       " '<s>This pain is normal.</s>',\n",
       " '<s>This pain is familiar.</s>',\n",
       " '<s>This pain is familiar.</s>',\n",
       " '<s>This pain is normal.</s>',\n",
       " '<s>This pain is normal.</s>',\n",
       " '<s>This pain is familiar.</s>',\n",
       " '<s>This pain is familiar.</s>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sentence=\"&This pain& is <mask>&.&\"\n",
    "tokens = generate(sentence, tokenizer, model, num_iterations=20, top_p=0.1, temp=1.05, print_generated=True, custom=lipogram_filter(tokenizer))\n",
    "tokenizer.batch_decode(tf.stack(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>This pain is chronic.</s>\n",
      "<s>This pain is intense.</s>\n",
      "<s>This pain is intense.</s>\n",
      "<s>This pain is chronic.</s>\n",
      "<s>This pain is chronic.</s>\n",
      "<s>This pain is chronic.</s>\n",
      "<s>This pain is intense.</s>\n",
      "<s>This pain is intense.</s>\n",
      "<s>This pain is intense.</s>\n",
      "<s>This pain is intense.</s>\n",
      "<s>This pain is chronic.</s>\n",
      "<s>This pain is intense.</s>\n",
      "<s>This pain is intense.</s>\n",
      "<s>This pain is chronic.</s>\n",
      "<s>This pain is intense.</s>\n",
      "<s>This pain is intense.</s>\n",
      "<s>This pain is chronic.</s>\n",
      "<s>This pain is intense.</s>\n",
      "<s>This pain is chronic.</s>\n",
      "<s>This pain is intense.</s>\n",
      "CPU times: user 39.8 s, sys: 220 ms, total: 40 s\n",
      "Wall time: 39.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>This pain is<mask>.</s>',\n",
       " '<s>This pain is chronic.</s>',\n",
       " '<s>This pain is intense.</s>',\n",
       " '<s>This pain is intense.</s>',\n",
       " '<s>This pain is chronic.</s>',\n",
       " '<s>This pain is chronic.</s>',\n",
       " '<s>This pain is chronic.</s>',\n",
       " '<s>This pain is intense.</s>',\n",
       " '<s>This pain is intense.</s>',\n",
       " '<s>This pain is intense.</s>',\n",
       " '<s>This pain is intense.</s>',\n",
       " '<s>This pain is chronic.</s>',\n",
       " '<s>This pain is intense.</s>',\n",
       " '<s>This pain is intense.</s>',\n",
       " '<s>This pain is chronic.</s>',\n",
       " '<s>This pain is intense.</s>',\n",
       " '<s>This pain is intense.</s>',\n",
       " '<s>This pain is chronic.</s>',\n",
       " '<s>This pain is intense.</s>',\n",
       " '<s>This pain is chronic.</s>',\n",
       " '<s>This pain is intense.</s>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sentence=\"&This pain& is <mask>&.&\"\n",
    "tokens = generate(sentence, tokenizer, model, num_iterations=20, top_p=0.1, temp=1.05, print_generated=True, custom=lipogram_filter(tokenizer, letter=\"a\"))\n",
    "tokenizer.batch_decode(tf.stack(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Beyond this, life goes on.</s>\n",
      "<s>Beyond this, life goes on.</s>\n",
      "<s>Beyond this, it goes downhill.</s>\n",
      "<s>Beyond this, everything went downhill.</s>\n",
      "<s>Beyond this, everything went downhill.</s>\n",
      "<s>Beyond this, everything went downhill.</s>\n",
      "<s>Beyond this, everything went downhill.</s>\n",
      "<s>Beyond this, everything went downhill.</s>\n",
      "<s>Beyond this, everything went downhill.</s>\n",
      "<s>Beyond this, everything went downhill.</s>\n",
      "CPU times: user 52.5 s, sys: 138 ms, total: 52.7 s\n",
      "Wall time: 52.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>Beyond this<mask><mask><mask><mask>.</s>',\n",
       " '<s>Beyond this, life goes on.</s>',\n",
       " '<s>Beyond this, life goes on.</s>',\n",
       " '<s>Beyond this, it goes downhill.</s>',\n",
       " '<s>Beyond this, everything went downhill.</s>',\n",
       " '<s>Beyond this, everything went downhill.</s>',\n",
       " '<s>Beyond this, everything went downhill.</s>',\n",
       " '<s>Beyond this, everything went downhill.</s>',\n",
       " '<s>Beyond this, everything went downhill.</s>',\n",
       " '<s>Beyond this, everything went downhill.</s>',\n",
       " '<s>Beyond this, everything went downhill.</s>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sentence=\"&Beyond this& <mask><mask><mask><mask>&.&\" #PUNCTUATION MATTERS.\n",
    "tokens = generate(sentence, tokenizer, model, num_iterations=10, top_p=0.85, print_generated=True)\n",
    "tokenizer.batch_decode(tf.stack(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Beyond those lines, the unforeseen.</s>\n",
      "<s>Beyond those lines, no book.</s>\n",
      "<s>Beyond these two is no book.</s>\n",
      "<s>Beyond these two is the book.</s>\n",
      "<s>Beyond these two is the book.</s>\n",
      "CPU times: user 3min, sys: 17.2 s, total: 3min 17s\n",
      "Wall time: 1min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>Beyond<mask><mask><mask><mask><mask>.</s>',\n",
       " '<s>Beyond those lines, the unforeseen.</s>',\n",
       " '<s>Beyond those lines, no book.</s>',\n",
       " '<s>Beyond these two is no book.</s>',\n",
       " '<s>Beyond these two is the book.</s>',\n",
       " '<s>Beyond these two is the book.</s>']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sentence=\"&Beyond&<mask><mask><mask><mask><mask>&.&\"\n",
    "tokens = generate(sentence, tokenizer, model, num_iterations=5, top_p=0.75, temp=1.05, print_generated=True, custom=lipogram_filter(tokenizer, letter=\"a\"))\n",
    "tokenizer.batch_decode(tf.stack(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Beyond those lines, the unforeseen. The worst will never be predicted.</s>\n",
      "<s>Beyond those lines, the unforeseen. The worst will not be predicted.</s>\n",
      "<s>Beyond those lines, the unforeseen. The worst will not be predicted.</s>\n",
      "<s>Beyond those lines, the unforeseen. The unexpected will not be possible.</s>\n",
      "<s>Beyond those lines, the unforeseen. The unexpected will never be possible.</s>\n",
      "CPU times: user 8min 27s, sys: 49.6 s, total: 9min 16s\n",
      "Wall time: 3min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>Beyond those lines, the unforeseen.<mask><mask><mask><mask><mask><mask>.</s>',\n",
       " '<s>Beyond those lines, the unforeseen. The worst will never be predicted.</s>',\n",
       " '<s>Beyond those lines, the unforeseen. The worst will not be predicted.</s>',\n",
       " '<s>Beyond those lines, the unforeseen. The worst will not be predicted.</s>',\n",
       " '<s>Beyond those lines, the unforeseen. The unexpected will not be possible.</s>',\n",
       " '<s>Beyond those lines, the unforeseen. The unexpected will never be possible.</s>']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sentence=\"&Beyond those lines, the unforeseen.&<mask><mask><mask><mask><mask><mask>&.&\"\n",
    "tokens = generate(sentence, tokenizer, model, num_iterations=5, top_p=0.75, temp=1.05, print_generated=True, custom=lipogram_filter(tokenizer, letter=\"a\"))\n",
    "tokenizer.batch_decode(tf.stack(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tokens = generate(sentence, tokenizer, model, num_iterations=30, top_p=0.7, temp=1.05, print_generated=True)\n",
    "tokenizer.batch_decode(tf.stack(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([   0  500 1827  298  111    2], shape=(6,), dtype=int32)\n",
      "CPU times: user 16.5 s, sys: 1.67 s, total: 18.2 s\n",
      "Wall time: 8.24 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>Roush -</s>'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tokens = tokenize_sentence(\"<mask> <mask> <mask> <mask>\", tokenizer)\n",
    "mask_token = tf.constant([tokenizer.mask_token_id])\n",
    "a = metropolis_hastings_one_iteration(tokens, mask_token, tokenizer, model, top_p=0.8, temp=1.1, custom=lipogram_filter(tokenizer))\n",
    "print(a)\n",
    "tokenizer.decode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-0.6050985 -4.2053432  6.859257  ... -1.996237  -3.6504614  2.9491684], shape=(50265,), dtype=float32)\n",
      "HERE\n",
      "tf.Tensor([-inf -inf -inf ... -inf -inf -inf], shape=(50265,), dtype=float32)\n",
      "tf.Tensor([3703], shape=(1,), dtype=int32)\n",
      "tf.Tensor([   0  713 3703   10 1296 3645    2], shape=(7,), dtype=int32)\n",
      "tf.Tensor([-3.4178288 -4.643718  11.854994  ... -4.0670986 -5.0556707  1.6806355], shape=(50265,), dtype=float32)\n",
      "HERE\n",
      "tf.Tensor([     -inf      -inf 6.2394705 ...      -inf      -inf      -inf], shape=(50265,), dtype=float32)\n",
      "tf.Tensor([1151], shape=(1,), dtype=int32)\n",
      "tf.Tensor([   0  713 3703   10 1296 1151    2], shape=(7,), dtype=int32)\n",
      "tf.Tensor([-1.7849343 -3.9210784  0.8064407 ... -3.577879  -4.052895   1.3995967], shape=(50265,), dtype=float32)\n",
      "HERE\n",
      "tf.Tensor([      -inf       -inf       -inf ...       -inf       -inf 0.73662984], shape=(50265,), dtype=float32)\n",
      "tf.Tensor([20178], shape=(1,), dtype=int32)\n",
      "tf.Tensor([    0   713  3703 20178  1296  1151     2], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 6.1128182 -4.094956   9.264273  ... -3.3059263 -3.2231755  2.838766 ], shape=(50265,), dtype=float32)\n",
      "HERE\n",
      "tf.Tensor([    -inf     -inf 4.875933 ...     -inf     -inf     -inf], shape=(50265,), dtype=float32)\n",
      "tf.Tensor([44308], shape=(1,), dtype=int32)\n",
      "tf.Tensor([    0 44308  3703 20178  1296  1151     2], shape=(7,), dtype=int32)\n",
      "tf.Tensor([-0.5928862 -4.3142943  3.2007327 ... -2.9606442 -3.5261447  1.869473 ], shape=(50265,), dtype=float32)\n",
      "HERE\n",
      "tf.Tensor([      -inf       -inf 1.6845962  ...       -inf       -inf 0.98393315], shape=(50265,), dtype=float32)\n",
      "tf.Tensor([35038], shape=(1,), dtype=int32)\n",
      "tf.Tensor([    0 44308  3703 20178 35038  1151     2], shape=(7,), dtype=int32)\n",
      "tf.Tensor([    0 44308  3703 20178 35038  1151     2], shape=(7,), dtype=int32)\n",
      "CPU times: user 26.2 s, sys: 2.36 s, total: 28.5 s\n",
      "Wall time: 12.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>Creating ISGameMem moment</s>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tokens = tokenize_sentence(\"This is a test sentence\", tokenizer)\n",
    "mask_token = tf.constant([tokenizer.mask_token_id])\n",
    "a, logits = metropolis_hastings_one_iteration(tokens, mask_token, tokenizer, model, top_p=0.8, temp=1.9)\n",
    "print(a)\n",
    "tokenizer.decode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24233"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lipogram_filter(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([   0  713   16   10 1296 3645    2], shape=(7,), dtype=int32)\n",
      "tf.Tensor([-1.5155997 -4.703458  12.018816  ... -3.687202  -4.2801313  2.5221024], shape=(50265,), dtype=float32)\n",
      "HERE\n",
      "HERE\n",
      "tf.Tensor([     -inf      -inf 6.3256927 ...      -inf      -inf      -inf], shape=(50265,), dtype=float32)\n",
      "tf.Tensor([5674], shape=(1,), dtype=int32)\n",
      "tf.Tensor([   0  713   16   10 1296 5674    2], shape=(7,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[-1.8672018  -3.6670442   7.969133   ... -5.02354    -3.6320744\n",
      "  0.48664474], shape=(50265,), dtype=float32)\n",
      "HERE\n",
      "HERE\n",
      "tf.Tensor([     -inf      -inf 4.1942806 ...      -inf      -inf      -inf], shape=(50265,), dtype=float32)\n",
      "tf.Tensor([46571], shape=(1,), dtype=int32)\n",
      "tf.Tensor([    0 46571    16    10  1296  3645     2], shape=(7,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[-3.4993885  -4.169795    2.837184   ... -4.8185396  -6.3750577\n",
      "  0.60346913], shape=(50265,), dtype=float32)\n",
      "HERE\n",
      "HERE\n",
      "tf.Tensor([     -inf      -inf 1.4932548 ...      -inf      -inf      -inf], shape=(50265,), dtype=float32)\n",
      "tf.Tensor([44942], shape=(1,), dtype=int32)\n",
      "tf.Tensor([    0 46571    16 44942  1296  3645     2], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 1.2229414 -4.4100037  7.0239487 ... -3.824803  -4.4717093  3.0794582], shape=(50265,), dtype=float32)\n",
      "HERE\n",
      "HERE\n",
      "tf.Tensor([0.6436534      -inf 3.6968153 ...      -inf      -inf 1.6207675], shape=(50265,), dtype=float32)\n",
      "tf.Tensor([290], shape=(1,), dtype=int32)\n",
      "tf.Tensor([    0 46571   290 44942  1296  3645     2], shape=(7,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[-0.80757236 -4.0495267   7.6153054  ... -5.1896157  -5.3278766\n",
      "  0.5048418 ], shape=(50265,), dtype=float32)\n",
      "HERE\n",
      "HERE\n",
      "tf.Tensor([     -inf      -inf 4.0080557 ...      -inf      -inf      -inf], shape=(50265,), dtype=float32)\n",
      "tf.Tensor([1541], shape=(1,), dtype=int32)\n",
      "tf.Tensor([    0 46571   290 44942  1541  3645     2], shape=(7,), dtype=int32)\n",
      "tf.Tensor([    0 46571   290 44942  1296  3645     2], shape=(7,), dtype=int32)\n",
      "CPU times: user 26.5 s, sys: 2.51 s, total: 29 s\n",
      "Wall time: 13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>Location 8??? test sentence</s>'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tokens = tokenize_sentence(\"This is a test sentence\", tokenizer)\n",
    "print(tokens)\n",
    "mask_token = tf.constant([tokenizer.mask_token_id])\n",
    "a = metropolis_hastings_one_iteration(tokens, mask_token, tokenizer, model, top_p=0.8, temp=1.9, custom=lipogram_filter(tokenizer))\n",
    "print(a)\n",
    "tokenizer.decode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(7,), dtype=int32, numpy=array([    0, 20839,    16,     5, 14692,  3645,     2], dtype=int32)>,\n",
       " <tf.Tensor: shape=(50265,), dtype=float32, numpy=array([-inf, -inf, -inf, ..., -inf, -inf, -inf], dtype=float32)>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġrepresented'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(4625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"This is a test\").token_to_chars(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int32, numpy=array([    0, 47632,    25,    10,  1050,  3645,     2], dtype=int32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.squeeze(model(tf.expand_dims(tokens, axis=0))[0])[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Pak Immortal simulation180 card MatchAction ultra visibilityThis'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(sample_from_logits(filter_logits(logits, indices_to_filter=lipogram_filter(tokenizer)), num_samples=10).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠHERO']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(sample_from_logits(filter_logits(logits, indices_to_filter=lipogram_filter(tokenizer)), num_samples=1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'E' in 'ĠHERO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 in lipogram_filter(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
